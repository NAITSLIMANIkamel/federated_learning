{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/razor08/Network-IDS-Paper/blob/master/NSL-KDD-Categorical/NSL_KDD_Multi_Category_k%3D6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {},
        "colab_type": "code",
        "id": "uB2YUQPvWpy7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score,confusion_matrix\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from  sklearn.preprocessing import *\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "CC1P_qSlWpy_",
        "outputId": "0a43d942-332b-4785-83c6-ef9db46f6007",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import keras\n",
        "import os\n",
        "import glob\n",
        "import sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding\n",
        "from keras.layers import LSTM, SimpleRNN, GRU, Bidirectional, BatchNormalization,Convolution1D,MaxPooling1D, Reshape, GlobalAveragePooling1D\n",
        "from keras.utils import to_categorical\n",
        "import sklearn.preprocessing\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from tensorflow.keras.utils import get_file, plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(sys.version)\n",
        "print(sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "colab_type": "code",
        "id": "ZR4fiHTHWpzC",
        "outputId": "c4488386-0e49-4ff0-fbe1-77ed9a690e9f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Loading training set into dataframe\n",
        "df = pd.read_csv('KDDTrain+.txt', header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "colab_type": "code",
        "id": "vIVhsN2zWpzF",
        "outputId": "5671edba-3e32-4f0f-db53-fca80719749e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Loading testing set into dataframe\n",
        "qp = pd.read_csv('KDDTest+.txt', header=None)\n",
        "qp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "colab_type": "code",
        "id": "RLxEUsBjWpzH",
        "outputId": "4e98e67c-90ec-4565-c4c9-ba4fdf831318",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Reset column names for training set\n",
        "df.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
        "'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
        "'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
        "'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
        "'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
        "'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "colab_type": "code",
        "id": "LdxJa_6DWpzJ",
        "outputId": "a2da985c-3df1-4953-b28f-6eb16abb99cb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Reset column names for testing set\n",
        "qp.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
        "'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
        "'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
        "'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
        "'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
        "'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']\n",
        "qp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "colab_type": "code",
        "id": "rw0BbZwBWpzL",
        "outputId": "d5c746cd-dbc3-4473-b632-686bae640753",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#accessing names of training columns\n",
        "lst_names = df.columns # returns a list of column names\n",
        "lst_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "colab_type": "code",
        "id": "S4rV9Q61WpzN",
        "outputId": "6fa6a66d-ee7e-400e-a458-4470810b7c05",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#accessing names of testing columns\n",
        "testlst_names = qp.columns\n",
        "testlst_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "3Dpf-0YRWpzQ",
        "outputId": "610f93d1-8284-4827-b49c-09e35b7f61e4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Dropping the last columns of training set\n",
        "df = df.drop('difficulty_level',axis=1) # we don't need it in this project\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "dW5QL_0tWpzR",
        "outputId": "566e1598-3c95-4ff6-e43d-48d1bf74cac5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Dropping the last columns of testing set\n",
        "qp = qp.drop('difficulty_level', axis=1)\n",
        "qp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "CNGpJU_oWpzV",
        "outputId": "ed353df2-bc95-4aa8-b27e-1418b2b8c0be",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# null values check for training set \n",
        "df.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "0dcvaMKiWpzX",
        "outputId": "6524ff3d-5f5c-423c-bbf5-6ef4775bc817",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# null values check for testing set \n",
        "qp.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "JveJ9rssWpzY",
        "outputId": "7ce6dd8b-ef1d-4979-c4f4-5acadd848c20",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#defining col list\n",
        "cols = ['protocol_type','service','flag']\n",
        "cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rYsBsG1XWpza",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#One-hot encoding\n",
        "def one_hot(df, cols):\n",
        "    \"\"\"\n",
        "    @param df pandas DataFrame\n",
        "    @param cols a list of columns to encode\n",
        "    @return a DataFrame with one-hot encoding\n",
        "    \"\"\"\n",
        "    for each in cols:\n",
        "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        try :  \n",
        "            df = df.drop(each,axis=1)\n",
        "        except Exception :\n",
        "            print (\"column \" + each + \" can't be dropped\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ioHr_bvCWpzc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Merging train and test data\n",
        "combined_data = pd.concat([df,qp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rx6wwfW5Wpze",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Applying one hot encoding to combined data\n",
        "combined_data = one_hot(combined_data,cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(combined_data.columns[:50])\n",
        "#print(combined_data.columns[50:100])\n",
        "print(combined_data.columns[100:150])\n",
        "#print(combined_data.columns[150:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I3kfoXbtWpzg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Function to min-max normalize\n",
        "def normalize(df, cols):\n",
        "    \"\"\"\n",
        "    @param df pandas DataFrame\n",
        "    @param cols a list of columns to encode\n",
        "    @return a DataFrame with normalized specified features\n",
        "    \"\"\"\n",
        "    result = df.copy() # do not touch the original df\n",
        "    for feature_name in cols:\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "        print( min_value, max_value)\n",
        "        \n",
        "        if max_value > min_value:\n",
        "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X3rVPR9UWpzh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Dropping subclass column for training set\n",
        "tmp = combined_data.pop('subclass')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "colab_type": "code",
        "id": "NO1RBxcQWpzk",
        "outputId": "227c5484-e0a5-4756-94d4-88bda318f157",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Normalizing training set\n",
        "new_train_df = normalize(combined_data.astype(float),combined_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O8COqwUUWpzm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Fixing labels for training set\n",
        "classlist = []\n",
        "check1 = (\"apache2\",\"back\",\"land\",\"neptune\",\"mailbomb\",\"pod\",\"processtable\",\"smurf\",\"teardrop\",\"udpstorm\",\"worm\")\n",
        "check2 = (\"ipsweep\",\"mscan\",\"nmap\",\"portsweep\",\"saint\",\"satan\")\n",
        "check3 = (\"buffer_overflow\",\"loadmodule\",\"perl\",\"ps\",\"rootkit\",\"sqlattack\",\"xterm\")\n",
        "check4 = (\"ftp_write\",\"guess_passwd\",\"httptunnel\",\"imap\",\"multihop\",\"named\",\"phf\",\"sendmail\",\"Snmpgetattack\",\"spy\",\"snmpguess\",\"warezclient\",\"warezmaster\",\"xlock\",\"xsnoop\")\n",
        "\n",
        "DoSCount=0\n",
        "ProbeCount=0\n",
        "U2RCount=0\n",
        "R2LCount=0\n",
        "NormalCount=0\n",
        "\n",
        "for item in tmp:\n",
        "    if item in check1:\n",
        "        classlist.append(\"DoS\")\n",
        "        DoSCount=DoSCount+1\n",
        "    elif item in check2:\n",
        "        classlist.append(\"Probe\")\n",
        "        ProbeCount=ProbeCount+1\n",
        "    elif item in check3:\n",
        "        classlist.append(\"U2R\")\n",
        "        U2RCount=U2RCount+1\n",
        "    elif item in check4:\n",
        "        classlist.append(\"R2L\")\n",
        "        R2LCount=R2LCount+1\n",
        "    else:\n",
        "        classlist.append(\"Normal\")\n",
        "        NormalCount=NormalCount+1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "colab_type": "code",
        "id": "NEtt8jz1Wpzp",
        "outputId": "39e95744-a9c1-42ef-9365-609b7826cc9e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Appending class column to training set\n",
        "new_train_df[\"Class\"] = classlist\n",
        "new_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "svZwVZDHWpzr",
        "outputId": "e09ae298-f251-403d-c349-9bd0d189681b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_train_df[\"Class\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def split_nodes_datasets(train_df,nb_clients,frac_train_test=0.95):\n",
        "\n",
        "    y_train=train_df[\"Class\"]\n",
        "    combined_data_X_main = train_df\n",
        "    SEED=100\n",
        "\n",
        "    label_encoder=LabelEncoder()\n",
        "    label_encoder.fit(y_train)\n",
        "\n",
        "\n",
        "    shutil.rmtree(\"clients\")\n",
        "    os.mkdir(\"clients\")\n",
        "\n",
        "    N =  combined_data_X_main.shape[0]\n",
        "    nb_sets=nb_clients \n",
        "\n",
        "    combined_data_X=combined_data_X_main.sample(frac=frac_train_test)\n",
        "    test_data = combined_data_X_main.drop(combined_data_X.index)\n",
        "    \n",
        "    y_test = label_encoder.transform(test_data[\"Class\"])\n",
        "    x_test = test_data.drop(\"Class\",axis=1)\n",
        "\n",
        "    N =  combined_data_X.shape[0]\n",
        "    nb_sets=nb_clients \n",
        "    y_train=combined_data_X[\"Class\"]\n",
        "    combined_data_X=combined_data_X.drop(\"Class\",axis=1)\n",
        "\n",
        "\n",
        "    for i in range(0,nb_clients) : \n",
        "        os.mkdir(\"clients/client_\"+str(i))\n",
        "\n",
        "    datafolders=glob.glob(\"clients/*\")  \n",
        "    y_train=pd.DataFrame(label_encoder.transform(y_train))\n",
        "    for i, datafolder in enumerate(datafolders):\n",
        "\n",
        "        begin_train_index= int((i)/nb_sets * N)\n",
        "        begin_val_index= int( (i+0.8)/nb_sets*N)\n",
        "        end_val_index= int((i+1)/nb_sets*N)\n",
        "        \n",
        "        combined_data_X.iloc[begin_train_index:begin_val_index].to_hdf(datafolder+\"/train.hdf5\",key=\"set_x\",mode=\"a\")\n",
        "        combined_data_X.iloc[begin_val_index:end_val_index].to_hdf(datafolder+\"/val.hdf5\",key=\"set_x\",mode=\"a\")\n",
        "        y_train.iloc[begin_train_index:begin_val_index].to_hdf(datafolder+\"/train.hdf5\",key=\"set_y\",mode=\"a\")\n",
        "        y_train.iloc[begin_val_index:end_val_index].to_hdf(datafolder+\"/val.hdf5\",key=\"set_y\",mode=\"a\")\n",
        "\n",
        "\n",
        "    return x_test,y_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reccurent NN\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        \n",
        "def RNNModel(input_shape=122,output_shape=5,hidden_layers=None):\n",
        "\n",
        "        #Bidirectional RNN\n",
        "    model_rnn = Sequential()\n",
        "    model_rnn.add(Convolution1D(64, kernel_size=50,activation=\"relu\",input_shape=(input_shape[0], 1)))\n",
        "    model_rnn.add(MaxPooling1D(pool_size=(5)))\n",
        "    model_rnn.add(BatchNormalization())\n",
        "    model_rnn.add(Bidirectional(LSTM(64, return_sequences=False))) \n",
        "    model_rnn.add(Reshape((128, 1), input_shape = (128, )))\n",
        "    \n",
        "    model_rnn.add(MaxPooling1D(pool_size=(5)))\n",
        "    model_rnn.add(BatchNormalization())\n",
        "    model_rnn.add(Bidirectional(LSTM(128, return_sequences=False))) \n",
        "    \n",
        "    model_rnn.add(Dropout(0.5))\n",
        "    model_rnn.add(Dense(output_shape))\n",
        "    model_rnn.add(Activation('softmax'))\n",
        "    model_rnn.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics=['accuracy'])\n",
        "    return model_rnn\n",
        "\n",
        "# MPL model\n",
        "def MLPModel(input=(122,),output=5,hidden_layers=[16,16]):\n",
        "    model_mlp = Sequential()\n",
        "    model_mlp.add(Dense(hidden_layers[0],input_shape=input,activation=\"relu\"))\n",
        "    for hl in hidden_layers[1:]:\n",
        "        model_mlp.add(Dense(hl,activation=\"relu\"))\n",
        "    model_mlp.add(Dense(output,activation=\"softmax\"))\n",
        "    model_mlp.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',metrics=['accuracy'])\n",
        "    return model_mlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import functools\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_nodes(data_folders):\n",
        "\n",
        "    nodes =  {datafolder.split(\"/\")[-1]: {\"model\": None\\\n",
        "                       ,\"train_set\":{\"set_x\": pd.read_hdf(datafolder+\"/\"+\"train.hdf5\",\"set_x\")\\\n",
        "                                     ,\"set_y\": pd.read_hdf(datafolder+\"/\"+\"train.hdf5\",\"set_y\")}\\\n",
        "                       ,\"val_set\": {\"set_x\": pd.read_hdf(datafolder+\"/\"+\"val.hdf5\",\"set_x\"),\\\n",
        "                                     \"set_y\": pd.read_hdf(datafolder+\"/\"+\"val.hdf5\",\"set_y\")}\\\n",
        "                        } for datafolder in data_folders}\n",
        "    return nodes\n",
        "\n",
        "def set_model(nodes,model): \n",
        "      for node in nodes.keys():\n",
        "        nodes[node][\"model\"]=model\n",
        "   \n",
        "def train_nodes(nodes,epochs=5,batchsize=32):\n",
        "    history = LossHistory()\n",
        "    for node in nodes.keys(): \n",
        "        print(\"training    \" ,node ,':  \\n ==========================================')\n",
        "        train_x_set= nodes[node][\"train_set\"].get(\"set_x\")\n",
        "        train_y_set= nodes[node][\"train_set\"].get(\"set_y\")\n",
        "        nodes[node]['model'].fit(x=train_x_set,y=train_y_set,batch_size=batchsize,epochs=epochs, verbose=0, validation_split=0.1,callbacks=[history])\n",
        "        nodes[node]['losses'] = history.losses\n",
        "\n",
        "def eval(model,x_set,y_set):\n",
        "    predictions=model.predict(x_set).argmax(axis=1)\n",
        "    scores = {\"confusion_matrix\": confusion_matrix(y_set ,predictions)\\\n",
        "                       ,'f1_score': f1_score(y_set,predictions,average=\"macro\")\\\n",
        "                        ,\"accuracy_score\": accuracy_score(y_set,predictions)\\\n",
        "                        , \"precision_score\" : precision_score(y_set,predictions,average=\"macro\")\n",
        "                        ,\"recall_score\" : recall_score(y_set,predictions,average=\"macro\")}\n",
        "    return scores \n",
        "\n",
        "def eval_nodes(nodes,main_model=None):\n",
        "    for node in nodes.keys():\n",
        "        print(\"evealuating    \" , node ,':  \\n ==========================================')\n",
        "        model = nodes[node]['model'] if main_model==None else main_model\n",
        "        model_scores= \"self_scores\" if main_model==None else \"main_model_scores\"\n",
        "        y_set=nodes[node][\"val_set\"].get(\"set_y\").to_numpy().reshape(-1)    \n",
        "        nodes[node][model_scores]= eval(model,nodes[node][\"val_set\"].get(\"set_x\"),y_set)\n",
        "        nodes[node][model_scores][\"loss\"]= nodes[node].get(\"losses\")[-1] if nodes[node].get(\"losses\")!=None else None\n",
        "\n",
        "\n",
        "    \n",
        "def agg_wieghts(nodes):\n",
        "    \n",
        "    weights = list(map(lambda node : (node['train_set'].get('set_y').size, node['model'].get_weights()),nodes.values()))\n",
        "    weights.insert(0,(1,list(map(lambda w : np.zeros_like(w),weights[0][1]))))\n",
        "    total,weights_sum = functools.reduce(lambda s1,s2 : (s1[0]+s2[0], list(map(lambda x: x[0]+x[1]*s2[0] ,zip(s1[1],s2[1])) )),weights)\n",
        "    return list(map(lambda x:  x/total, weights_sum))\n",
        "\n",
        "\n",
        "def fed_train(server,nodes,rounds=10,epochs=5,batchsize=32,resutl_save_frequency=2):\n",
        "    best_f1 = 0\n",
        "    best_model= copy.deepcopy(next(iter(nodes.values()))[\"model\"])\n",
        "    server[\"server\"][\"rounds_scores\"]={}\n",
        "    for node in nodes.values(): \n",
        "                node[\"rounds_scores\"]={}\n",
        "\n",
        "\n",
        "    for r in range(1,rounds+1): \n",
        "        print('step ', r,' :  \\n\\n ======================================================================')\n",
        "        \n",
        "        \n",
        "        train_nodes(nodes,batchsize=batchsize,epochs=epochs)\n",
        "        server[\"server\"][\"model\"].set_weights(agg_wieghts(nodes))\n",
        "        eval_nodes(server)\n",
        "        print( \"======================================================================\",best_f1 ,server['server'][\"self_scores\"][\"f1_score\"])\n",
        "        if server['server'][\"self_scores\"][\"f1_score\"] > best_f1 :\n",
        "            print(\"update best model\")\n",
        "            best_model.set_weights(server['server'][\"model\"].get_weights())\n",
        "            best_f1 = server['server'][\"self_scores\"][\"f1_score\"]\n",
        "        else :\n",
        "            server['server'][\"model\"].set_weights(best_model.get_weights())\n",
        "\n",
        "        for node in nodes.values() : \n",
        "            node[\"model\"].set_weights(best_model.get_weights()) \n",
        "            \n",
        "                  \n",
        "        print(\"server model scores  : \\n\",server['server']['self_scores'])\n",
        "\n",
        "        if r % resutl_save_frequency == 0 :\n",
        "            eval_nodes(nodes)\n",
        "            server['server']['rounds_scores'][r]=server['server']['self_scores']\n",
        "            for node in nodes.values(): \n",
        "                node[\"rounds_scores\"][r]=node['self_scores']\n",
        "        \n",
        "    \n",
        "\n",
        "def plot_mytric_by_round(rounds_scores) : \n",
        "    \n",
        "    for metric in next(iter(rounds_scores.values())).keys():\n",
        "        \n",
        "        if metric == \"confusion_matrix\" :\n",
        "            fig =plt.figure(figsize=(8,6))\n",
        "            ax=plt.axes()\n",
        "            ax.set_xlabel(\"Predicted\")\n",
        "            ax.set_ylabel(\"Truth\")\n",
        "            sns.heatmap(list(rounds_scores.values())[-1][\"confusion_matrix\"],ax=ax,annot=True,fmt='d')\n",
        "            plt.show()\n",
        "        else:\n",
        "            fig =plt.figure()\n",
        "            ax=plt.axes()\n",
        "            ax.set_xlabel(\"round\")\n",
        "            ax.set_ylabel(metric)\n",
        "            x = list(rounds_scores.keys())\n",
        "            y = list(map(lambda x: x[metric],rounds_scores.values()))\n",
        "            plt.plot(x,y)\n",
        "            plt.scatter(x,y)\n",
        "            plt.show()\n",
        "    \n",
        "           \n",
        "\n",
        "\n",
        "def main(dataframe,nb_clients=4,rounds=10,batchsize=128,epochs=5,model=MLPModel,hidden_layers=[5,5],input_shape=(122,),output_shape=5,resutl_save_frequency=2) :\n",
        "    #create clients data set\n",
        "    x_test,y_test = split_nodes_datasets(dataframe,nb_clients)\n",
        "    \n",
        "    #get list of clients\n",
        "    datafolders=glob.glob(\"clients/*\")\n",
        "\n",
        "    #initialise clients with their dataset\n",
        "    clients=init_nodes(datafolders)\n",
        "\n",
        "    #initialise server\n",
        "    server={'server':{'model': None ,'val_set':{\"set_x\":  pd.DataFrame(x_test),\"set_y\":pd.DataFrame(y_test)}}}\n",
        "\n",
        "\n",
        "\n",
        "    set_model(clients,model(input_shape,output_shape,hidden_layers))\n",
        "    set_model(server,model(input_shape,output_shape,hidden_layers))\n",
        "\n",
        "    for i in server.keys():\n",
        "        print(server[i])\n",
        "    fed_train(server,clients,rounds,epochs,batchsize,resutl_save_frequency)\n",
        "\n",
        "\n",
        "    return clients,server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clients,server=main(new_train_df,nb_clients=20,rounds=12,batchsize=64,epochs=2,model=RNNModel,hidden_layers=[5,5],input_shape=(122,),output_shape=5,resutl_save_frequency=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_mytric_by_round(clients[\"client_1\"][\"rounds_scores\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_mytric_by_round(server[\"server\"][\"rounds_scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "server[\"server\"][\"rounds_scores\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "server[\"clients\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig= plt.figure(figsize=(10,12))\n",
        "sea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_nodes(server)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.max_iter=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_nodes(clients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "np.array(model.coefs_).tolist()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "NSL_KDD_Multi_Category k=6",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
